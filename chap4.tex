\chapter{Methodology and Results}
The Riskmap system allows citizens to easily submit
disaster reports as such it has allowed the Urban Risk Lab at
MIT to gather thousands of reports of real flooding in Indonesia and India.
These data points include requests for help, traffic reports, indications that an
area is safe, and advice for other citizens in the area. Images attached to
reports include a wide variety of scenes, from daylight highways with cars and
motorcycles to night time deserted alleys. Additionally, citizens were asked to
provide an estimated flood height using a slider.

\section{System Design}
It is paramount for disaster systems to be highly available and scalable during
events. The REACT system has been designed such that it is modular and ready to
scale.

\subsection{Configuration}
A single configuration file allows for easy customization of the underlying 
training data. \figureautorefname{}~\ref{fig:config} shows two default configurations 
that include all of the data from Chennai and Jakarta in 2017. Other 
configurations allow the opportunity to choose only those reports that 
include an image, or those that come from a specific social network.
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{images/config.jpg}
    \caption{Configuration Abstraction}
    \label{fig:config}
\end{figure}

\subsection{Data Loaders}
Data Loaders are constructed by passing a configuration that implements the 
GenericConfig interface. They are then responsible for implementing methods 
that allow for retrieving the three kinds of data that the REACT system 
uses for prediction: text, image, and flood depth estimation. While the
RiskmapLoader is the only Loader that is currently implemented, it was 
important to separate data loaders in order to make REACT ready for future change.
% TODO talk about japan?
% TODO CITE :https://www.gov-online.go.jp/eng/publicity/book/hlj/html/201803/201803_03_en.html

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{images/loaders.jpg}
    \caption{Adaptable Data Loaders}
    \label{fig:data_loaders}
\end{figure}

\subsection{Labelers}
Data embeddings, or labels, turn human recognizable data into feature vectors
that can be used for machine learning. There are many choices for how to 
label the disaster data in REACT, as such any Labeler that implements the 
GenericLabeler interface can be used seamlessly in the system as shown in \figureautorefname{}~\ref{fig:labelers}. The AwsLabeler uses AWS Rekognition 
to create feature vectors, while the BowLabeler uses a bag of words approach 
to encode textual information. An IdentityLabeler is provided in order to 
facilitate the passing of raw features.
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{images/labelers.png}
    \caption{Many different labelers can be used to encode data in REACT}
    \label{fig:labelers}
\end{figure}

\subsection{Learners}
Different learning methods are represented by different implementations of
the GenericLearner interface as shown in \figureautorefname{}~\ref{fig:labelers}. The perceptron algorithm and the Support Vector
Machine method are implemented in REACT as simple linear classifiers. An 
IdentityLearner is also provided to act as a pass through in case raw 
features are desired.
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{images/learners.png}
    \caption{The Learner abstraction makes it easy to switch from one method to another}
    \label{fig:learners}
\end{figure}

\section{Text}
As shown in Section~\ref{chap1:riskmap}, the Riskmap system allows citizens to provide
a textual description to emergency managers. In Indonesia, most of the reports
are provided in Bahasa, the local language; however, in Chennai all reports were
submitted in English even though the system also supports Tamil. In both Chennai
and Jakarta these reports are quite brief, with the longest reports having 140
characters. In this manner, they are quite similar to tweets which were
initially 140 characters but this limit was doubled in 2017. Helpfully, this
means that much of the work described in Section~\ref{chap3:text} applies to the Riskmap
text corpus.

Table~\ref{table:text_sample} contains a sample of ten reports that are
indicative of those found in the Riskmap textual descriptions.

\begin{table}
  \begin{tabular}{ll}
    \toprule
    pkey & text \\ 
    \hline
    \midrule
    169  &
    Waterlogging near cathedral road flyover  \\
    171  &
    1st street Engineers avenue \\
    173  &
    Not that much water safe only \\
    174  &
    50cm water stagnant on the road \\
    176  &
    Water level rising  slowly  \\
    177  &
    Water logging  \\
    178  &
    Model school road is completely flooded, with water almost knee deep \\
    179  &
    Heavy rain in West mambalam flood \\
    180  &
    Water on roads. Stay safe \\
    182  &                                                           4cm
    rainfall.. still continuing.. hope for safe .. dont come outside in
    night time \\
    181  &
    Luz signal flooded knee deep water \\
    \bottomrule
  \end{tabular}
\label{table:text_sample}
  \caption{A representative selection of report texts}
\end{table}

\subsection{Preprocessing}
We first remove test reports, which are reports submitted in order to ensure
that the system is working. The following POSTGRESQL query was executed to remove
reports that were only used to test the system:

\begin{lstlisting}[language=SQL]
SELECT   pkey,
         text
FROM     riskmap.all_reports
WHERE    text IS NOT NULL
AND      Length (text) > 0
AND      text NOT similar TO '%%(T|t)(E|e)(S|s)(T|t)%%'
ORDER BY created_at;
\end{lstlisting}

We then use python to remove punctuation and then split along whitespace, thus
splitting the source text into individual words without any spaces.

\begin{lstlisting}[language=python]
def prepare_text(report_text):
    '''
    returns a list of strings where each string is a different word
    '''
    import string
    exclude = set(string.punctuation)
    s = "".join(ch for ch in inp if ch not in exclude )
    return s.lower().split()
\end{lstlisting}

\subsection{Sentiment analysis}
Since each report in the Chennai dataset includes a textual description in English,
we could use off the shelf sentiment analysis to gauge how
negatively citizens are feeling. It might be the case that a highly negative
sentiment corresponds to heavy flooding and that a positive sentiment
corresponds to lighter or no flooding.  We can investigate the relation between
a negative sentiment and heavy flooding by using conditional probability. We set
the threshhold for negative sentiment at~.5 and then use the AWS Rekognition API
in order to classify texts into heavy flooding when negative sentiment is
greater than $.5$ and into light or no flooding otherwise.

We use bayes' rule in order to analyze the true positive rate--- the
probability that a report represents heavy flooding given a negative sentiment:
\\
$$P( Heavy Flooding | negative) = $$
$$\frac{P(negative | Heavy Flooding)*P(HeavyFlooding)}{P(negative)} =.65 $$
\\

The false positive rate, which is the probability that there is no heavy
flooding given a negative sentiment is given by:

\\
$$P( No Flooding | negative) =  \frac{P(negative | NoFlooding)*P(NoFlooding)}{P(negative)} =.34 $$
\\

These probabilities show that while there is some relation between a negative
sentiment and flooding, it is not a very strong signal. Furthermore, there are
no off the shelf sentiment analysis tools for the Indonesian language, so a
model based on AWS Rekogniton or Google Cloud Natural Language API would not
translate to the Jakarta dataset.

\subsection{Bag Of Words}
While sentiment analysis might not be a strong enough signal of heavy flooding/
no heavy flooding, our experiment shows that the textual data contains important
information. In order to train a machine learning algorithm on textual data one
must first create an embedding that maps natural language into feature vectors.
There are many ways of creating embeddings as discussed in
Section~\ref{chap3:text}, but many of them require large datasets or do not
support Indonesian. For example, word2vec is a popular embedding model that
produces floating point vectors and has achieved remarkable performance;
however, the size of its training vocabulary is 962,000 unique
words~\cite{mikolovDistributedRepresentationsWords2013}. It is possible to
download a pre-trained word2vec model and use it to encode new texts, but such a
pre-trained model doesn't exist for Indonesian. We could train it using a
different dataset of Indonesian texts, but there is no guarantee that our domain
specific words would have a good embedding after having trained with a different
corpus.

The bag of words encoding is particularly attractive to the Riskmap
dataset because it language agnostic and can therefore work on both the Chennai
and Jakarta datasets.  The bag of words approach to classifying texts consists
of first creating a vocabulary that maps from a token t to a unique index i. Each
report text is then encoded into a feature vector by setting the ith element to
1 if the token t exists in the report
text~\cite{khuranaNaturalLanguageProcessing2017}.

The bag of words model correctly classifies 67 percent of reports in the Chennai corpus
under 5 fold cross validation. Examining the data, we see that there are many
instances of reports such as `no flooding here' which are being
misclassified because the embedding is not able to understand relationships
between adjacent words.

\subsection{Bigrams}
Bigrams are an embedding that allows the separator to learn relationships
between adjacent words. The vocabulary is created by using pairs of adjacent
words, such that `no flooding here' would turn into 2 tokens: `no flooding' and
`flooding here'. Embedding the text data from the Riskmap system into a bigram
vector creates a very large vector (over 3 times the size of the unitary approach),
and only improves accuracy by .02 on average, as such we chose not to use a
bigram embedding.

%TODO: graph of svm/ perceptron cross validation performance

\section{Images}
\subsection{Transfer Learning}
Image classification as described in Section~\ref{chap3:image} traditionally uses a
deep convolutional neural network and requires millions of images. Since both the
Chennai and Jakarta datasets are quite small (<5000 images), it would not be
feasible to create and train our own CNN. Transfer learning uses a pretrained
image classifier trained on a different dataset in order to assign labels that the
original network was not trained on.
We used the Resnet18 architecture pretrained on ImageNet in order to
experiment with transfer learning.

\begin{tabular}{llr}\label{table:transfer_learning}
\toprule
      dataset &             method &  best validation acc. \\
 Jakarta 2017 &           Full net &                  0.60 \\
\midrule
 Jakarta 2017 &  Feature extractor &                  0.61 \\
 Chennai 2017 &           Full net &                  0.62 \\
 Chennai 2017 &  Feature extractor &                  0.62 \\
\bottomrule
\end{tabular}

\subsection{Using Machine Learning as a Service}
% motivation -
Results from popular machine learning as a service providers have a `flooding'
label whereas publicly available pretrained networks do not.
As Google and AWS improve their own models, a classifier based on these results
will also improve without the need to invest large compute resources.
\subsubsection{AWS Image Rekognition}
% here's what an image and the top 10 labels look like

\subsubsection{Google Cloud Vision AI}
% here's what an image and the top 10 labels look like

\subsubsection{Results}
When the dataset size is limited and it is difficult to gather new data, 
it is possible to estimate future true error rates by using
cross validation. Cross validation consists of splitting the dataset into
k many groups. The learning model is then trained on k-1 groups and tested 
against the held out group. The average error over all k groups is then 
taken as an estimate of the true error rate of the model.
%TODO: table of perceptron and svm cross validation

With a balanced hold out set comprising of ten percent of the
data points, the Support Vector Machine (SVM) linear classifier achieves a score
upwards of \.70 as shown in \tablename{}~\ref{chap4:imageSvmPerceptron}.
Labels from AWS Rekognition were used because AWS returned many more labels than 
Google did for the same image. More labels means that there are more dimensions
for the classifier to differentiate between heavy flooding and no heavy flooding
images.

\begin{table}[htbp]
\caption{Image Classification Accuracy}
\label{chap4:imageSvmPerceptron}
\begin{tabular}{lllr}
\toprule
      Dataset &      Method &           Validation Method &  Validation Acc. \\
 Jakarta 2017 &  Perceptron &     held out validation set &             0.70 \\
\midrule
 Jakarta 2017 &         SVM &     held out validation set &             0.71 \\
 Chennai 2017 &  Perceptron &  five fold cross validation &             0.83 \\
 Chennai 2017 &         SVM &  five fold cross validation &             0.84 \\
\bottomrule
\end{tabular}
\end{table}

% with zero labels and without

\subsection{Visual Bag of Words}
~\cite{yangEvaluatingBagofvisualwordsRepresentations2007}

\section{Flood Height}
\subsection{Raw}
\subsection{Normalized}
\section{Ensemble with Neural Net}
In~\cite{jordanHierarchicalMixturesExperts1994}, Jordan and Jacobs showed that
neural networks can be effectively used to vote between different classifiers
that are effective only in their specific domain of the overall space. They present a
case for using the Estimation Maximization (EM) algorithm for optimizing the
weights of the neural network.
Hierarchical mixtures of experts and the EM
algorithm~\cite{jordanHierarchicalMixturesExperts1994}
+ bishop p. 673~\cite{bishopPatternRecognitionMachine2006}

It has been shown that ensemble learning methods that pick between several
experts can provide accurate prediction even if the experts have low accuracy.
By combining weak experts, we were able to achieve upwards of 85\% accuracy on a
randomly sample held out validation set comprised of ten percent of the data
points.

%TODO: table giving cross validation on ensemble learning

.74 all data 
.77 with picture
 
 with pic plus report 

\subsection{Validation Scores}
